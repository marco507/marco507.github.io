<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>SQL &#8211; BigDataTime</title>
	<atom:link href="https://marco507.github.io/category/sql/feed/" rel="self" type="application/rss+xml" />
	<link>https://marco507.github.io/</link>
	<description>Programming, AI and Data Science</description>
	<lastBuildDate>Sat, 16 Dec 2023 18:31:09 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.2</generator>

<image>
	<url>https://marco507.github.io/wp-content/uploads/2023/07/cropped-bigdatatime-favicon-3-32x32.png</url>
	<title>SQL &#8211; BigDataTime</title>
	<link>https://marco507.github.io/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Analysing Website Technologies &#8211; A Complete Data Science Project</title>
		<link>https://marco507.github.io/analysing-website-technologies-a-complete-data-science-project/</link>
		
		<dc:creator><![CDATA[Marco Schweiss]]></dc:creator>
		<pubDate>Fri, 15 Dec 2023 18:18:30 +0000</pubDate>
				<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Python]]></category>
		<category><![CDATA[SQL]]></category>
		<guid isPermaLink="false">https://marco507.github.io/?p=714</guid>

					<description><![CDATA[As the digital landscape continues to evolve at a rapid pace, understanding the technologies that power websites has become increasingly crucial. From front-end frameworks to [...]]]></description>
										<content:encoded><![CDATA[<p>As the digital landscape continues to evolve at a rapid pace, understanding the technologies that power websites has become increasingly crucial. From front-end frameworks to server-side languages, the array of tools and platforms available can be overwhelming for businesses and developers alike. In this data science project, we will do an analysis of website technologies to uncover insights that can inform strategic decision-making in the ever-changing world of web development. The benefits of such an analysis could be for example a database of possible business leads or having a basis for a data-driven hiring strategy.</p>
<p>The scope of this project ranges across the whole data science workflow, from raw data acquisition to reporting of the results. The list below highlights the individual milestones of the project, which we will discuss in detail later on:</p>
<ol>
<li>Compiling a reasonably sized dataset of website links with the corresponding industry branch and the technologies used to run the web presence</li>
<li>Wrangling the data into an accessible format that can be used for further analysis</li>
<li>Creation of an interactive web-based report featuring self-service analytics that can be used by decision makers</li>
</ol>
<h2>Gathering the dataset</h2>
<p>If you ask data professionals about their favorite task, the answer will most certainly be scraping of unstructured web data, which can be considered the modern equivalent of working in a 19th century coal mine. Of course, web scraping was also the starting point for this project, with our first contact point being a well-known public database of Austrian companies. The idea here was to leverage their already existing categorization of websites into industry branches, which we can use for example to spot possible differences in technology choice. Maybe a web agency is specifically targeting retail businesses and want to exactly know their prevalent technology choices. By using the categories, we can add another layer of granularity to our dataset and answer such questions.</p>
<p>Gathering the data is done with a Python script that utilizes several libraries, including requests, BeautifulSoup, Wappalyzer, and fire. It uses the requests library to make an HTTP GET request and checks the status code of the response. If the status code is 200, indicating a successful request, the HTML content is returned. If the status code is 404, the script prints a message and skips the site. For status codes 400 and 500, the script continues the loop to retry the request. This method ensures that the script handles different scenarios when fetching web pages.</p>
<p>Overall, the script is designed to scrape multiple pages of each industry branch from the company database until a threshold of 1000 websites including technology stacks are gathered for each branch. The method iterates over the pagination, gets the URL for each company profile, and fetches the HTML content like discussed above. It then uses BeautifulSoup to parse the HTML and extract the actual website link for a company from the profile page. If a website link is found, the script uses the Wappalyzer library to analyze the technology stack of the website. The results are then written to a CSV file.</p>
<p>We always check if the actual website link returns a 200 status code so that we do not scrape broken links. Additionally, the fire library is employed to create a command-line interface for the script to allow running multiple instances of the script in tandem. Theoretically, we could use multiple threads, but having a dedicated terminal and status bar makes the whole process more manageable. To facilitate comprehension, I included a diagram of the scraping process, which can be seen <a href="https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-scraping_flow.jpg" target="_blank" rel="noopener">here</a>.</p>
<h2>Wrangling and database creation</h2>
<p>At the end of the data acquisition, we now have ten different CSV files (one for each branch), with each one containing 1000 websites and their technologies. The next steps of the project are concerned with bringing the raw data into a suitable format for the final analysis and visualization. First we load each of the ten CSV files and convert them into a single Pandas DataFrame which looks as following:</p>
<p><img fetchpriority="high" decoding="async" class="size-full wp-image-729 aligncenter" src="https://marco507.github.io/wp-content/uploads/2023/12/techstack-analysis_dataframe.png" alt="" width="690" height="200" srcset="https://marco507.github.io/wp-content/uploads/2023/12/techstack-analysis_dataframe.png 690w, https://marco507.github.io/wp-content/uploads/2023/12/techstack-analysis_dataframe-300x87.png 300w" sizes="(max-width: 690px) 100vw, 690px" /></p>
<p>If we examine the &#8220;Techstack&#8221; column, we can see dictionaries containing the technologies. These dictionaries are thereby stored as strings. Now we have multiple possibilities to format our data, for example we could split our dictionaries and store each link, branch and technology combination as a distinct record in the DataFrame. Considering that we will also enrich our data with additional categories for the technologies and that a technology can have multiple categories, we quickly get a lot of redundant data to store. If we flatten out all possible combination, we blow up our DataFrame from 10000 to around 60000 records.</p>
<p>This would not be an issue for this small dataset, but if we want to incrementally add new data in the future, this would lead to issues at some point in time. To alleviate this problem, we will create a dedicated SQL database in third normal form to minimize the storage of redundant data. This database will then serve as the basis for our analysis and the web-based report. The ER-diagram below describes the schema that we will implement:</p>
<p><img decoding="async" class="size-full wp-image-730 aligncenter" src="https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-er_diagram.jpg" alt="" width="490" height="401" srcset="https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-er_diagram.jpg 490w, https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-er_diagram-300x246.jpg 300w" sizes="(max-width: 490px) 100vw, 490px" /></p>
<p>We create the database with the Sqlite3 library starting by creating a schema and following we fill the &#8220;Website&#8221;, &#8220;Technology&#8221; and &#8220;Category&#8221; table. The next step is also relatively straightforward, where we use loops and checks to fill up the so-called join tables named &#8220;Website_Technology&#8221; and &#8220;Technology_Category&#8221;. If you ask why the schema is set up in this form, consider the following example: A website has jQuery, WordPress and PHP in their technology stack. Regarding the categories of each technology, we have &#8220;JavaScript&#8221; library for jQuery, &#8220;Programming Language&#8221; for PHP and for WordPress we have two categories, namely &#8220;Blog&#8221; and &#8220;CMS&#8221; (Content Management System).</p>
<p>Now instead of creating an individual record for each possible combination, we just fill the &#8220;Website&#8221;, &#8220;Technology&#8221; and &#8220;Category&#8221; table with the respective entities and then create an entry for each relation in the join tables. So going back to our example, we would create three entries in the &#8220;Website_Technology&#8221; table, with all entries pointing at the website and each entry pointing at one of the technologies. Now we do the same for the &#8220;Technology_Category&#8221; table, and we can effectively reduce redundant data storage to a minimum. Additionally, we get the convenience of using SQL for our analysis in the next step</p>
<h2>Deploying the Report</h2>
<p>To follow suit with current state-of-the-art solutions for data visualization like Power BI or Tableau, we will create an interactive report with some basic self-service analytics. To do this, we use the Streamlit framework, which is optimized for fast development of data-apps with Python. The architecture of the web application is thereby relatively straightforward, because we basically connect standard front-end components like sliders, drop down menus and forms with SQL commands that interface with the database. With the additional integration of plotting capabilities in Streamlit, we can then build very neat data dashboards in a reasonable time. A quick overview of the architecture can be seen below:</p>
<p><img decoding="async" class="size-full wp-image-736 aligncenter" src="https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-web_app.jpg" alt="" width="511" height="116" srcset="https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-web_app.jpg 511w, https://marco507.github.io/wp-content/uploads/2023/12/techstack_analysis-web_app-300x68.jpg 300w" sizes="(max-width: 511px) 100vw, 511px" /></p>
<p>Now there is not much left to say about the web app except that it can be viewed with the following link, so please take a look for yourself. -&gt; <a href="https://techstack-analysis.streamlit.app/">Techstack Analysis Report</a></p>
<h2>Conclusion</h2>
<p>In this article, we looked at a project that encompassed the whole data science workflow, from gathering raw data to the creation of an interactive report that could be used by decision makers to guide their strategies. Such a project features a lot of different technologies and techniques that must be orchestrated to deliver a usable final result. Nonetheless, this usage of different tools for creative problem-solving, is probably the most exciting part of data science. As always, the code for this project can be found on GitHub in the following <a href="https://github.com/marco507/Techstack-Analysis-Report" target="_blank" rel="noopener">repository</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Creating an Airbnb-like Data Mart</title>
		<link>https://marco507.github.io/creating-an-airbnb-like-data-mart/</link>
		
		<dc:creator><![CDATA[Marco Schweiss]]></dc:creator>
		<pubDate>Mon, 10 Jul 2023 08:17:40 +0000</pubDate>
				<category><![CDATA[App Development]]></category>
		<category><![CDATA[SQL]]></category>
		<guid isPermaLink="false">https://marco507.github.io/?p=214</guid>

					<description><![CDATA[Airbnb was founded in October 2007, stemming from personal experiences of encountering overpriced shared apartments and fully booked hotels during a crowded conference in San [...]]]></description>
										<content:encoded><![CDATA[<p>Airbnb was founded in October 2007, stemming from personal experiences of encountering overpriced shared apartments and fully booked hotels during a crowded conference in San Francisco. The original name, Airbedandbreakfast, was later shortened to Airbnb in 2009. Serving as an online platform, Airbnb facilitates the connection between hosts and guests and assumes responsibility for managing the booking process.</p>
<p>Transactions are conducted through the platform, with guests making payments for their bookings using credit cards. To ensure that the accommodation matches its description on the platform, hosts receive payment 24 hours after the guest&#8217;s arrival. In 2013, Airbnb generated revenue by charging a commission of 6-12% from guests, and 3% from hosts, and amassed a total of $150 million from approximately 10 million overnight stays.</p>
<p>Every user, whether a host or a guest, creates a profile page on Airbnb. Hosts are required to upload at least one photo and provide a phone number, while guests need to provide more comprehensive information. Hosts have the option to describe their accommodations using text and photos. Both guests and hosts can rate each other based on their experiences. The platform also features a calculator function that allows users to estimate their expected income from their own accommodations.</p>
<p>To improve my SQL skills, I created a proof-of-concept for the Airbnb use case in the form of a data mart with the following list of requirements:</p>
<ul>
<li><strong>Entity Relationship Model (ERM):</strong> Development of an ERM that encompasses the definition of data tables, attributes, and relationships among entities pertaining to the Airbnb use case.</li>
<li><strong>Database Management System (DBMS): </strong>Selection of a suitable Database Management System (DBMS), such as MySQL or any other SQL-based system, to implement the database for handling Airbnb-related information.</li>
<li><strong>Database Structure: </strong>Establishment of an appropriate database structure that facilitates efficient storage and processing of data relevant to the Airbnb use case.</li>
<li><strong>Dummy Data:</strong> Generation of realistic dummy data to populate the database, ensuring that it accurately reflects the various scenarios and characteristics encountered in real-world Airbnb situations.</li>
<li><strong>Query Functionality:</strong> Creation of well-crafted queries that enable seamless retrieval and presentation of data, effectively showcasing the functionalities of the database.</li>
<li><strong>Documentation:</strong> Thorough documentation of all implementation steps, including the development of the ERM, database structure, and SQL statements employed, ensuring comprehensive records of the entire process.</li>
<li><strong>Data Storage Optimization:</strong> Implementation of data storage optimization techniques, including proper normalization of the database structure, to minimize redundancy and optimize data storage for essential information.</li>
</ul>
<p>In the following sections, we will take a look at how the project was conceptualized and developed.</p>
<h2>Conception</h2>
<p>The first step of the project was to derive an appropriate database design. The database design process includes a crucial stage called database modeling, which holds the utmost significance in the overall database design. Any oversights or omissions during this step can have a detrimental impact on the implementation stage, potentially rendering the database useless.</p>
<p>To initiate the process, the first step involves crafting a comprehensive requirements specification for the Airbnb project. This specification document should encompass a thorough analysis of the requirements, delving into the following aspects in greater detail:</p>
<ul>
<li>Identification of various roles or user groups involved in the system.</li>
<li>Specification of the actions or activities performed by each of these roles.</li>
<li>Determination of the necessary data and functions required to fulfill the system&#8217;s requirements.</li>
</ul>
<p>The results are compiled in this <a href="https://marco507.github.io/wp-content/uploads/2023/07/data-mart_concept.pdf" target="_blank" rel="noopener">PDF</a> file.</p>
<h2>Development</h2>
<p>The next step was to implement the data mart. Throughout this phase, I ensured that all SQL statements were meticulously documented in my database file.</p>
<p>Specifically, I delivered the tables and relations for the database in a SQL data file, aligning them with the concepts outlined in the ER-Diagram. Additionally, I diligently documented every SQL statement involved in the creation process.</p>
<p>To ensure a comprehensive test of the system, I made certain that each table contained a minimum of 20 entries. Furthermore, I created at least one test case for the database, specifically designed to validate its functionality based on the ER-Model.</p>
<p>By adhering to the provided guidelines and ensuring the documentation and testing of the system, a first working version of the data mart was derived. A detailed explanation of the database design and implementation procedure can be seen in this <a href="https://marco507.github.io/wp-content/uploads/2023/07/data-mart_development.pdf" target="_blank" rel="noopener">PDF</a>.</p>
<h2>Finalization</h2>
<p>In this final phase, the goal was to polish and refine the database management system, after having received feedback. Given that only a minor change to the schema of the database was necessary and the first iteration of the data mart was up to standard, this last phase was rather short. This <a href="https://marco507.github.io/wp-content/uploads/2023/07/data-mart_finalization.pdf" target="_blank" rel="noopener">document</a> sums up the project and shows the changes to the database schema.</p>
<h2>Conclusion</h2>
<p>As for my last application, this project was a great opportunity to gain development experience and also to make a deep dive into pure SQL. I really like the final version of the data mart and especially the location search functionality. The code for the data mart and instructions on how to use it can be found in this <a href="https://github.com/marco507/Airbnb-Data-Mart">GitHub repository</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
