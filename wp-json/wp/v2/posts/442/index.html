{"id":442,"date":"2023-07-17T15:42:35","date_gmt":"2023-07-17T13:42:35","guid":{"rendered":"https:\/\/bigdatatime.eu\/?p=442"},"modified":"2023-11-16T17:54:17","modified_gmt":"2023-11-16T16:54:17","slug":"pdf-summarization-and-flashcard-generation-with-chatgpt","status":"publish","type":"post","link":"https:\/\/bigdatatime.eu\/pdf-summarization-and-flashcard-generation-with-chatgpt\/","title":{"rendered":"PDF Summarization and Flashcard Generation with ChatGPT"},"content":{"rendered":"<p>In an era where information overload is prevalent, finding efficient ways to extract key insights from vast amounts of data has become crucial. As an aspiring student navigating the sea of academic papers during my time at university, I often found myself spending countless hours reading and analyzing research articles. Motivated by the desire to streamline this process, I embarked on a journey to develop a PDF summarizer using ChatGPT. Additionally, I wanted to leverage ChatGPTs language capabilities to automate the process of generating flashcards from the learning material.<\/p>\n<p>By leveraging the power of OpenAI&#8217;s state-of-the-art language model, I aimed to create an automated tool that would help me and others extract essential knowledge from lengthy scholarly texts. The primary objective was to facilitate efficient research and enhance the learning experience for students like myself.<\/p>\n<p>In this article, I will delve into the intricacies of building a PDF summarizer and flashcard generator with ChatGPT. I will discuss the underlying methods and the challenges encountered along the way.<\/p>\n<h2>The Portable Document Format<\/h2>\n<p>PDF, short for Portable Document Format, has become one of the most widely used file formats for document sharing and distribution. Its popularity is attributed to its ability to preserve the original formatting of a document across various platforms. However, this format presents unique challenges when it comes to extracting information and transforming it into a structured format.<\/p>\n<p>The main hurdle lies in the fact that PDFs store data as a series of graphical elements rather than plain text. While these graphical elements maintain the visual appearance of the document, they pose difficulties for programs aiming to extract meaningful content automatically. Directly accessing and parsing text from a PDF can be complex due to variations in layout, fonts, and non-standard encoding schemes.<\/p>\n<p>Another challenge is that PDFs often contain complex structures such as tables, headings, footnotes, citations, and images. These elements require specialized algorithms and techniques for accurate understanding and extraction. Moreover, despite efforts towards standardized file formats like tagged PDFs (PDF\/A), many documents are still produced without such metadata, making it even more challenging to extract relevant information systematically.<\/p>\n<p>To overcome these obstacles and bring the data within PDFs into a structured format, one must employ smart techniques such as optical character recognition (OCR) to convert graphical elements back into text data. Additionally, algorithms need to analyze the layout structure to identify headings, sections, paragraphs, and other textual components accurately, which brings us to the PDF Extract API from Adobe.<\/p>\n<h3>PDF Extract API<\/h3>\n<p>The PDF Extract API is a cloud-based service to automatically extract content and structural information from PDF documents. It can extract text, tables, and figures from both native and scanned PDFs. When extracting text, the API breaks it down into contextual blocks like paragraphs, headings, lists, and footnotes. It includes font, styling, and formatting information, providing comprehensive output. Tables within the PDF are extracted and parsed, including contents and table formatting for each cell. The API can also extract figures or images from the PDF, saving them as individual PNG files for further use.<\/p>\n<p>We will leverage this API to create a JSON file containing the PDF content and in turn, use this JSON file to create text files that we will feed into ChatGPT for further processing.<\/p>\n<h2>The Problem with Summarization<\/h2>\n<p>Text summarization is a natural language processing technique aimed at condensing large bodies of text into shorter summaries while still capturing the essential information. With the rise of transformer models like ChatGPT, extractive and abstractive summarization methods have seen significant advancements.<\/p>\n<p>However, when dealing with long-form content such as PDF documents, transformers face challenges due to their limited input length. Transformers typically have a maximum token limit, necessitating the truncation or splitting of longer texts. This approach can lead to the loss of important context and affect the quality of the summary.<\/p>\n<p>To address this limitation and preserve information integrity, we will leverage the headings present in the PDF documents. Headings provide structural organization to the text and can serve as natural breakpoints for splitting the document into smaller sections. By splitting the document based on headings, we can ensure that each section is more coherent and self-contained, resulting in better summarization outcomes.<\/p>\n<h2>PDF to JSON<\/h2>\n<p>As already explained the first step of our pipeline is the conversion of a raw PDF into a JSON file and the extraction of tables and figures. We will use the paper <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_paper.pdf\" target=\"_blank\" rel=\"noopener\">Attention is All You Need<\/a> by Vaswani and colleagues which is an important paper about transformer model architecture with over 80 000 citations for testing our approach. If you open the PDF you can see that we have a perfect candidate with a variety of elements such as headings, sub-headings, lists, math formulas, images, footnotes, and tables.<\/p>\n<p>For a detailed explanation of the JSON files structure please refer to the <a href=\"https:\/\/developer.adobe.com\/document-services\/docs\/overview\/pdf-extract-api\/howtos\/extract-api\/\">documentation<\/a> of the API provider. Sending the PDF to the Extract API returns the following files:<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter size-full wp-image-448\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_file-tree.jpg\" alt=\"\" width=\"237\" height=\"462\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_file-tree.jpg 237w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_file-tree-154x300.jpg 154w\" sizes=\"(max-width: 237px) 100vw, 237px\" \/><\/p>\n<p>From here on out we are confronted with three distinct problems:<\/p>\n<ol>\n<li>How do we get the information in the JSON file into a format suitable for our use case?<\/li>\n<li>How do we integrate the tables into the text files?<\/li>\n<li>How do we handle the figures?<\/li>\n<\/ol>\n<h2>JSON to Chapters<\/h2>\n<p>We will start by solving problem one which is the most straightforward to solve. By using the documentation we can iterate through the JSON file and transform the data to our target format, which is a folder of text files each corresponding to a section. A section is thereby everything that is contained between two headings.<\/p>\n<p>The second problem of integrating the tables back into the text files is also relatively easy to solve. When the JSON file indicates a table in its path attribute we read the table into a DataFrame, convert it into a CSV, and instead of saving the data to a file we save it inside our text file. By using | as column separators, ChatGPT is capable of understanding that this should be a table.<\/p>\n<p>The third part on our list is where it gets a bit more interesting. Let&#8217;s look at the following two images from the figure folder. Here is the first image<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-453\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_formula.png\" alt=\"\" width=\"483\" height=\"88\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_formula.png 680w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_formula-300x55.png 300w\" sizes=\"(max-width: 483px) 100vw, 483px\" \/><\/p>\n<p>and here is the second image:<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-454\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_diagram.png\" alt=\"\" width=\"200\" height=\"397\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_diagram.png 267w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_diagram-151x300.png 151w\" sizes=\"(max-width: 200px) 100vw, 200px\" \/><\/p>\n<p>Given that this application should also be used for generating flashcards, we need a way to tell apart diagrams and formulas and also to incorporate mathematical formulas into our text. We will omit the diagrams for the time being.<\/p>\n<h3>Training a Formula Detection Model<\/h3>\n<p>To solve the issue we will train a CNN on a dataset that I compiled from various different sources. We basically need a model that can distinguish between math formulas and everything else that could be included in a PDF. This can range from figures and diagrams to illustrations and images of everything else. The approach to training this model is the same as the one that I have already used for the <a href=\"https:\/\/bigdatatime.eu\/emotion-detection-in-images\/\" target=\"_blank\" rel=\"noopener\">facial emotion classifier<\/a>. Given the relatively simple task for the modified VGG19, because the two image categories are relatively distinct, we get an accuracy of around 99% which will have to suffice.<\/p>\n<p>Now we integrate this model into our initial script and every time the JSON file path attribute indicates a Figure, the model is called to predict whether we have a formula or not. Depending on the decision the figure is either skipped or sent to the <a href=\"https:\/\/mathpix.com\/\">Mathpix<\/a> API which returns us a Latex representation of the formula. The Latex code is then included in the text where it can be interpreted by ChatGPT.\u00a0 An example of a text file can be seen below:<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-textfile-1.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-457 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-textfile-1.jpg\" alt=\"\" width=\"796\" height=\"357\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-textfile-1.jpg 796w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-textfile-1-300x135.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-textfile-1-768x344.jpg 768w\" sizes=\"(max-width: 796px) 100vw, 796px\" \/><\/a><\/p>\n<h2>Chapters to Content<\/h2>\n<p>With the preprocessing of the PDFs done, we can now move on to the second big part of the pipeline which is the summarization and the creation of flashcards. As I have already mentioned we will use ChatGPT&#8217;s capabilities to do this. By using ChatGPT&#8217;s API we can improve our results by providing examples of how we want the task done by sending context in the form of a preceding conversation. To convert the information from the raw text files into high-quality summaries and flashcards we will incorporate an intermediate step that chunks the raw text data into blocks of related information. The image below shows the text of chapter eleven in a chunked format.<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-461 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks.jpg\" alt=\"\" width=\"1118\" height=\"580\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks.jpg 1118w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-300x156.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-1024x531.jpg 1024w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-768x398.jpg 768w\" sizes=\"(max-width: 1118px) 100vw, 1118px\" \/><\/a><\/p>\n<p>After chunking we then let ChatGPT create a summary<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-summary.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-463 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-summary.jpg\" alt=\"\" width=\"846\" height=\"285\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-summary.jpg 846w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-summary-300x101.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-summary-768x259.jpg 768w\" sizes=\"(max-width: 846px) 100vw, 846px\" \/><\/a><\/p>\n<p>and a list of flashcards:<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-flashcards.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-464 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-flashcards.jpg\" alt=\"\" width=\"846\" height=\"570\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-flashcards.jpg 846w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-flashcards-300x202.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-chunks-flashcards-768x517.jpg 768w\" sizes=\"(max-width: 846px) 100vw, 846px\" \/><\/a><\/p>\n<p>The image does not show all the flashcards that were created. The complete text file can be downloaded <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_chapter-11.txt\" target=\"_blank\" rel=\"noopener\">here<\/a>. As you may have noticed, the summary of the section is not really that much shorter than the original section. We will discuss how different lengths of text effects the summaries and the quality of the outputs later on. We also instruct ChatGPT to create flashcards from the summaries which gives us the following list:<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_summary-flashcards.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-467 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_summary-flashcards.jpg\" alt=\"\" width=\"846\" height=\"731\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_summary-flashcards.jpg 846w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_summary-flashcards-300x259.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_summary-flashcards-768x664.jpg 768w\" sizes=\"(max-width: 846px) 100vw, 846px\" \/><\/a><\/p>\n<h2>Content to Documents<\/h2>\n<p>The last step of the pipeline is to convert the text files into documents that can be further used for studying. We will first look into the CSV files that can be used in flashcard software like <a href=\"https:\/\/apps.ankiweb.net\/\">Anki<\/a>. Here we will create two CSV files one for the flashcards from the chunked files and one for the summary flashcards.<\/p>\n<p>In addition to the question and answer that makes up a flashcard, we also include the source (document name), the section heading, and the two combined inside a tags column, which is a sorting mechanism used by Anki. Given the correct settings inside Anki, we can import the <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/Attention-Is-All-You-Need_chunks.csv\">238 flashcards from the chunked text<\/a> and the <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/Attention-Is-All-You-Need_summary.csv\">113 flashcards from the summaries<\/a>. An example of how this looks inside Anki in its final form can be seen below:<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_anki.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-469 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_anki.jpg\" alt=\"\" width=\"848\" height=\"316\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_anki.jpg 848w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_anki-300x112.jpg 300w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_anki-768x286.jpg 768w\" sizes=\"(max-width: 848px) 100vw, 848px\" \/><\/a><\/p>\n<p>The second type of document which we will create is two Word documents that contain the <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/Attention-Is-All-You-Need_chunks.pdf\" target=\"_blank\" rel=\"noopener\">chunked<\/a> sections and the <a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/Attention-Is-All-You-Need_summaries.pdf\" target=\"_blank\" rel=\"noopener\">summarized<\/a> sections.<\/p>\n<h2>Discussion of the Results<\/h2>\n<p>Does our pipeline here perform flawlessly? No. Is it enough so that I could save a lot of time on tedious work by hand? Yes, absolutely.<\/p>\n<h3>Summaries<\/h3>\n<p>First of all, let&#8217;s talk about the rate of shrinkage that a document undergoes. The size of the resulting summary is dependent on how large the individual sections are and I think the GPT model handles this very well because larger sections get condensed more, and smaller sections that contain less information to begin with, do not get summarized too much. As you can see from our example, we have around 30% percent shrinkage of the original text, if we only consider the actual text as graphics, references and the abstract have been removed.<\/p>\n<p>On the other hand, I tested this pipeline on my written university lectures that have several hundred pages and here I often got a reduction of around 50% or more, of course depending on the structure of the document. So a rule of thumb is that a lot of text without much headings and graphics gets condensed more and a document with sparse text gets condensed less.<\/p>\n<p>The summaries that this pipeline generates can also be used as input for more summaries and solves the problem of too large input contexts, of course only if the original document is not too large to begin with. Here is an example of a one-page summary based on our five-page summary, that could be used for example to speed up a literature review :<\/p>\n<p><a href=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_one-page-summary.jpg\"><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-480 size-full\" src=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_one-page-summary.jpg\" alt=\"\" width=\"648\" height=\"570\" srcset=\"https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_one-page-summary.jpg 648w, https:\/\/bigdatatime.eu\/wp-content\/uploads\/2023\/07\/pdf-summarizer_one-page-summary-300x264.jpg 300w\" sizes=\"(max-width: 648px) 100vw, 648px\" \/><\/a><\/p>\n<p>Another idea would be to build a recursive summarizer where we first summarize sub-sections and then summarize these summaries into section-level summaries. Similar like OpenAI did in their <a href=\"https:\/\/openai.com\/research\/summarizing-books\">case study<\/a>. Regarding the quality of the summary I can of course only give you my subjective opinion as there is no standardized process to measure the quality of a summary. First I would like to explain how I use summaries in my overall approach to studying for a subject. The summary is used to get a first understanding of how the topics of a subject relate to each other and effectively reduces the time to do so by omitting detailed information.<\/p>\n<p>So in essence you can focus on building a higher-level overview of how topics relate and care for the details of a specific topic later on. Given that I studied successfully for several exams with this approach I am personally satisfied with the quality of the summaries and the time that the automated generation saved me.<\/p>\n<h3>Flashcards<\/h3>\n<p>Next, we will look into the flashcards. As you may have noticed, the flashcards are very granular which means that practically every bit of information is transformed into a question-and-answer format. The idea here is to curate the flashcards during the revision process, so you have built an understanding of what the learning material is about with the summaries, then studied the material in detail with the chunked and original text and finally revise it with the flashcards.<\/p>\n<p>During this revision process, you can then remove or rephrase sub-optimal cards and also check gaps in your existing knowledge. Additionally, you can evaluate the cards during studying which also can be considered a form of interleaved retrieval practice and results in better learning outcomes at least in my case. And last but not least the automated flashcard generation reduced the time it took to study for an exam, as I found curating the cards is faster than writing them from scratch. Of course, you can also just study the summary cards and call it a day.<\/p>\n<h2>Conclusion<\/h2>\n<p>In conclusion, the use of the PDF summarization pipeline built with ChatGPT has proven to be a game-changer in reducing study time and eliminating tedious handwriting work for me. The possibility for models like ChatGPT to act as a multitask-learner with minimal input is really fascinating and allows for a vast amount of use cases like this one. It is important to note that I use the GPT-3.5 model although GPT-4 is already available because the price per token demands it. A larger document with 100-200 pages costs around 2-3 $ at this time with the GPT-3.5 model and would cost 10 times as much with GPT-4.<\/p>\n<p>I already did tests with GPT-4 but the capabilities in regards to the transformation of text are not that much different between the two models to justify the costs. This also coincides with what OpenAI reported about the difference between the two models. Given the rapid advancements of this technology and the large number of tasks that can be solved with large-language models, I am looking forward to years of interesting application development. This time there will be no code on GitHub, if my coding skills interest you please refer to my other works.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>In an era where information overload is prevalent, finding efficient ways to extract key insights from vast amounts of data has become crucial. As an [&#8230;]<\/p>\n","protected":false},"author":2,"featured_media":493,"comment_status":"closed","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[6,8,1],"tags":[],"_links":{"self":[{"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/posts\/442"}],"collection":[{"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/comments?post=442"}],"version-history":[{"count":10,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/posts\/442\/revisions"}],"predecessor-version":[{"id":690,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/posts\/442\/revisions\/690"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/media\/493"}],"wp:attachment":[{"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/media?parent=442"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/categories?post=442"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/bigdatatime.eu\/wp-json\/wp\/v2\/tags?post=442"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}